#!/bin/bash
set -e

echo "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê"
echo "‚îÇ Alfred Agent Platform v2 - LLM Integration  ‚îÇ"
echo "‚îÇ Deployment Script                           ‚îÇ"
echo "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
echo ""

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
  echo "‚ùå Error: Docker is not running or not accessible"
  echo "Please start Docker and try again"
  exit 1
fi

# Ensure network exists
if ! docker network ls | grep -q alfred-network; then
  echo "üìå Creating alfred-network..."
  docker network create alfred-network
else
  echo "‚úÖ alfred-network already exists"
fi

# Build and start services
echo "üöÄ Deploying LLM integration services..."
docker-compose -f llm-integration-docker-compose.yml up -d --build

echo "‚è≥ Waiting for services to start..."
sleep 10

# Check service health
echo "üîç Checking service health..."

# Check Model Registry
if curl -s http://localhost:8079/health | grep -q "healthy"; then
  echo "‚úÖ Model Registry service is healthy"
else
  echo "‚ö†Ô∏è Model Registry service may not be fully initialized yet"
  echo "   Check logs with: docker logs alfred-model-registry"
fi

# Check Model Router
if curl -s http://localhost:8080/health | grep -q "healthy"; then
  echo "‚úÖ Model Router service is healthy"
else
  echo "‚ö†Ô∏è Model Router service may not be fully initialized yet"
  echo "   Check logs with: docker logs alfred-model-router"
fi

# Check Ollama
if curl -s http://localhost:11434/api/tags > /dev/null; then
  echo "‚úÖ Ollama service is ready"
else
  echo "‚ö†Ô∏è Ollama service may not be fully initialized yet"
  echo "   Check logs with: docker logs alfred-ollama"
fi

# Check Streamlit UI
if netstat -tuln | grep -q ":8502"; then
  echo "‚úÖ Streamlit UI is running"
else
  echo "‚ö†Ô∏è Streamlit UI may not be fully initialized yet"
  echo "   Check logs with: docker logs ui-chat"
fi

echo ""
echo "üìã Available services:"
echo "  - Model Registry: http://localhost:8079"
echo "  - Model Registry API Docs: http://localhost:8079/docs"
echo "  - Model Router: http://localhost:8080"
echo "  - Model Router API Docs: http://localhost:8080/docs"
echo "  - Ollama: http://localhost:11434"
echo "  - Streamlit UI: http://localhost:8502"
echo ""

echo "üìù Next steps:"
echo "  1. Setup Ollama models using the utility script:"
echo "     ./setup-ollama-models.sh"
echo ""
echo "  2. Manually register models if needed:"
echo "     curl -X POST http://localhost:8079/api/v1/models \\"
echo "       -H \"Content-Type: application/json\" \\"
echo "       -d '{\"name\":\"model_name\",\"display_name\":\"Model Display Name\",\"provider\":\"ollama\",\"model_type\":\"chat\",\"description\":\"Description\",\"capabilities\":[{\"capability\":\"text\",\"capability_score\":0.8}]}'"
echo ""
echo "  3. Trigger model discovery (note: may have issues with automatic discovery):"
echo "     curl -X POST http://localhost:8079/api/v1/api/v1/discovery/trigger"
echo ""
echo "  4. Check available models:"
echo "     curl http://localhost:8079/api/v1/models"
echo ""
echo "  5. For full documentation, see: LLM_INTEGRATION_GUIDE.md"
echo ""

echo "‚ú® Deployment completed!"