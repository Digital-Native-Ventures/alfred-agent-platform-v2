name: ML Trainer Benchmark

on:
  push:
    branches:
      - 'feat/ml-retrain-pipeline'
      - 'feat/alert-dataset-db'
      - 'feat/dynamic-threshold-opt'
      - 'feat/hf-transformers-integr'
      - 'feat/phase8.3-sprint4'
  pull_request:
    paths:
      - 'alfred/ml/**'
      - 'alfred/alerts/**'
      - 'backend/alfred/ml/**'
      - '.github/workflows/trainer-benchmark.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 3  # Must complete in 3 minutes
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: alerts
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    env:
      ALERT_DB_URI: postgresql://test:test@localhost:5432/alerts
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Cache ML models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-models-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-hf-models-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install ray[default] sentence-transformers psutil
      
      - name: Initialize test database
        env:
          PGPASSWORD: test
        run: |
          echo "Creating test alert data..."
          python -c "
          import os
          import json
          from sqlalchemy import create_engine, text
          from datetime import datetime, timedelta
          import random
          
          engine = create_engine(os.environ['ALERT_DB_URI'])
          
          # Create alerts table if not exists
          with engine.connect() as conn:
              conn.execute(text('''
                  CREATE TABLE IF NOT EXISTS alerts (
                      id SERIAL PRIMARY KEY,
                      message TEXT NOT NULL,
                      source VARCHAR(100),
                      severity VARCHAR(20),
                      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      metadata JSONB
                  )
              '''))
              conn.commit()
              
              # Insert test data
              alerts = []
              for i in range(1000):
                  severity = random.choice(['critical', 'warning', 'info'])
                  message = f'Test alert {i}: {severity} level event'
                  metadata = {'host': f'server-{i % 10}', 'app': f'app-{i % 5}'}
                  alerts.append({
                      'message': message,
                      'source': 'test',
                      'severity': severity,
                      'created_at': datetime.utcnow() - timedelta(hours=i),
                      'metadata': json.dumps(metadata)
                  })
              
              conn.execute(text('''
                  INSERT INTO alerts (message, source, severity, created_at, metadata)
                  VALUES (:message, :source, :severity, :created_at, :metadata::jsonb)
              '''), alerts)
              conn.commit()
          
          print('Test database initialized with 1000 alerts')
          "
      
      - name: Run training benchmark
        run: |
          echo "Running training benchmark..."
          python -c "
          import time
          import json
          import psutil
          import os
          from alfred.alerts.dataset import AlertDataset
          from alfred.alerts.trainer import AlertTrainer
          from alfred.ml.thresholds import ThresholdService
          
          # Benchmark dataset loading
          start_time = time.time()
          dataset = AlertDataset(os.environ['ALERT_DB_URI'])
          alerts = dataset.load_recent_alerts(1000)
          load_time = time.time() - start_time
          
          # Benchmark training
          trainer = AlertTrainer()
          start_time = time.time()
          model = trainer.train(alerts)
          train_time = time.time() - start_time
          
          # Benchmark inference
          test_alerts = dataset.load_recent_alerts(100)
          start_time = time.time()
          for alert in test_alerts:
              model.predict(alert)
          inference_time = (time.time() - start_time) / len(test_alerts) * 1000  # ms per alert
          
          # Memory usage
          process = psutil.Process()
          memory_usage = process.memory_info().rss / 1024 / 1024  # MB
          
          # Calculate noise reduction rate
          noise_count = sum(1 for a in test_alerts if model.predict(a) > 0.7)
          noise_rate = (len(test_alerts) - noise_count) / len(test_alerts)
          
          results = {
              'dataset_load_time': load_time,
              'training_time': train_time,
              'inference_latency_ms': inference_time,
              'memory_usage_mb': memory_usage,
              'noise_reduction_rate': noise_rate
          }
          
          with open('benchmark_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print(f'Dataset load time: {load_time:.2f}s')
          print(f'Training time: {train_time:.2f}s')
          print(f'Inference P99: {inference_time:.2f}ms')
          print(f'Memory usage: {memory_usage:.0f}MB')
          print(f'Noise reduction: {noise_rate*100:.1f}%')
          "
      
      - name: Check performance targets
        run: |
          python -c "
          import json
          
          with open('benchmark_results.json') as f:
              data = json.load(f)
          
          # Check performance targets
          load_time = data['dataset_load_time']
          latency = data['inference_latency_ms']
          memory = data['memory_usage_mb']
          noise_cut = data['noise_reduction_rate']
          
          print(f'Dataset load time: {load_time:.2f}s')
          print(f'Inference P99: {latency:.2f}ms')
          print(f'Memory usage: {memory:.0f}MB')
          print(f'Noise reduction: {noise_cut*100:.1f}%')
          
          # Enforce targets
          assert load_time < 10, f'Dataset load {load_time:.1f}s exceeds 10s target'
          assert latency < 15, f'Inference latency {latency:.1f}ms exceeds 15ms target'
          assert memory < 500, f'Memory usage {memory:.0f}MB exceeds 500MB target'
          assert noise_cut >= 0.45, f'Noise reduction {noise_cut*100:.1f}% below 45% target'
          
          print('✅ All performance targets met!')
          "
      
      - name: Run threshold optimization test
        run: |
          echo "Testing threshold optimization..."
          python -c "
          import json
          from alfred.ml.thresholds import ThresholdService
          
          # Test threshold optimization
          service = ThresholdService()
          initial = service.get_thresholds()
          
          # Simulate performance metrics
          metrics = {
              'false_positive_rate': 0.08,
              'accuracy': 0.92
          }
          
          optimized = service.optimize_thresholds(metrics)
          
          results = {
              'initial_thresholds': initial,
              'optimized_thresholds': optimized,
              'performance_metrics': metrics
          }
          
          with open('threshold_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print(f'Initial noise threshold: {initial[\"noise_threshold\"]}')
          print(f'Optimized noise threshold: {optimized[\"noise_threshold\"]}')
          print(f'Adjustment: {(optimized[\"noise_threshold\"] - initial[\"noise_threshold\"]) * 100:.1f}%')
          "
      
      - name: Upload benchmark report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: trainer-benchmark-report
          path: |
            benchmark_results.json
            threshold_results.json
            .benchmarks/
      
      - name: Post results to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const data = JSON.parse(fs.readFileSync('benchmark_results.json'));
            const thresholds = JSON.parse(fs.readFileSync('threshold_results.json'));
            
            const comment = `## 🎯 ML Trainer Benchmark Results
            
            ✅ Completed in < 3 minutes
            
            **Performance Metrics:**
            - 📦 Dataset Load: ${data.dataset_load_time.toFixed(2)}s (target < 10s)
            - 🏃 Training Time: ${data.training_time.toFixed(2)}s
            - ⚡ Inference P99: ${data.inference_latency_ms.toFixed(2)}ms (target < 15ms)
            - 🧠 Memory Usage: ${data.memory_usage_mb.toFixed(0)}MB (target < 500MB)
            - 🔇 Noise Reduction: ${(data.noise_reduction_rate * 100).toFixed(1)}% (target ≥ 45%)
            
            **Threshold Optimization:**
            - Initial Noise Threshold: ${thresholds.initial_thresholds.noise_threshold}
            - Optimized Noise Threshold: ${thresholds.optimized_thresholds.noise_threshold}
            - Adjustment: ${((thresholds.optimized_thresholds.noise_threshold - thresholds.initial_thresholds.noise_threshold) * 100).toFixed(1)}%
            
            [Download Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });